# Fase 4: Pinecone & Pipeline de Procesamiento

**Estado:** üîÑ EN PROGRESO (80% completada)  
**Duraci√≥n estimada:** 1.5-2 horas  
**Fecha inicio:** 2026-01-18

---

## Objetivo

Crear √≠ndice en Pinecone y pipeline completo para procesar art√≠culos JSON y subirlos como chunks vectorizados.

---

## ‚úÖ Completado

### 1. √çndice Creado en Pinecone

```bash
bash scripts/setup_index.sh
```

**Configuraci√≥n:**
```
Name: kb-articles-production
Dimension: 1024
Metric: cosine
Cloud: aws
Region: us-east-1
Model: llama-text-embed-v2
Field Map: text=content
State: Ready ‚úÖ
```

### 2. Scripts Creados

- ‚úÖ `scripts/setup_index.sh` - Crear √≠ndice
- ‚úÖ `data_pipeline/pinecone_uploader.py` - M√≥dulo de carga
- ‚úÖ `scripts/process_single_article.py` - Procesar 1 art√≠culo
- ‚úÖ `scripts/verify_article.py` - Verificar art√≠culo

---

## ‚ö†Ô∏è PROBLEMA ACTUAL

### Error al Subir Chunks

```
Vector dimension 0 does not match the dimension of the index 1024
```

### Causa

Los √≠ndices de Pinecone con **embeddings integrados** requieren formato especial de upsert que NO incluye vectores expl√≠citos.

**Lo que est√°bamos haciendo (INCORRECTO):**
```python
record = {
    "id": chunk["id"],
    "values": [],  # ‚ùå INCORRECTO para embeddings integrados
    "metadata": {...}
}
```

**Lo que necesitamos hacer (CORRECTO):**
```python
record = {
    "id": chunk["id"],
    "data": {
        "content": chunk["content"]  # ‚úÖ Pinecone genera embeddings de esto
    },
    "metadata": {
        # Metadata sin el content
    }
}
```

---

## üîß SOLUCI√ìN COMPLETA

### Paso 1: Corregir `pinecone_uploader.py`

**Archivo:** `data_pipeline/pinecone_uploader.py`

**Buscar el m√©todo `_upload_batch` (l√≠nea ~140):**

```python
def _upload_batch(self, batch: List[Dict[str, Any]]) -> bool:
    """Sube un batch de chunks con retry logic."""
    # Preparar records para Pinecone
    records = []
    for chunk in batch:
        record = {
            "id": chunk["id"],
            "values": [],  # ‚ùå ESTA L√çNEA ES EL PROBLEMA
            "metadata": {
                **chunk["metadata"],
                "content": chunk["content"]
            }
        }
        records.append(record)
    
    # ... resto del c√≥digo
```

**REEMPLAZAR CON:**

```python
def _upload_batch(self, batch: List[Dict[str, Any]]) -> bool:
    """
    Sube un batch de chunks con retry logic.
    
    IMPORTANTE: Para √≠ndices con embeddings integrados (model + field_map),
    Pinecone espera formato con 'data' en vez de 'values'.
    """
    # Preparar records para Pinecone con embeddings integrados
    records = []
    for chunk in batch:
        # Para embeddings integrados: usar 'data' con el contenido
        # Pinecone generar√° el embedding del campo 'content'
        record = {
            "id": chunk["id"],
            "data": {
                "content": chunk["content"]  # Campo que Pinecone embedir√°
            },
            "metadata": chunk["metadata"]  # Metadata SIN el content duplicado
        }
        records.append(record)
    
    # Intentar upload con retries
    for attempt in range(self.max_retries):
        try:
            # Upsert usando inference API (para embeddings integrados)
            self.index.upsert(
                vectors=records,
                namespace=self.namespace
            )
            return True
            
        except Exception as e:
            logger.warning(f"Intento {attempt + 1}/{self.max_retries} fall√≥: {e}")
            
            if attempt < self.max_retries - 1:
                time.sleep(self.retry_delay * (attempt + 1))
            else:
                logger.error(f"Batch fall√≥ despu√©s de {self.max_retries} intentos")
                return False
```

**CAMBIOS CLAVE:**
1. ‚ùå Eliminar `"values": []`
2. ‚úÖ Agregar `"data": {"content": chunk["content"]}`
3. ‚úÖ Metadata NO debe incluir content duplicado

---

### Paso 2: Probar la Correcci√≥n

```bash
cd kb-rag-system
source venv/bin/activate

# Procesar art√≠culo de prueba
python scripts/process_single_article.py \
  "../Participant Advisory/Distributions/LT: How to Request a 401(k) Termination Cash Withdrawal or Rollover.json"
```

**Output esperado:**
```
‚úÖ Art√≠culo cargado: LT: How to Request...
üî® Generando chunks...
‚úÖ 33 chunks generados
üì§ Subiendo chunks a Pinecone...
Uploading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00]
‚úÖ Todos los chunks se subieron exitosamente
‚úÖ PROCESAMIENTO COMPLETADO
```

---

### Paso 3: Verificar en Pinecone

```bash
python scripts/verify_article.py \
  "lt_request_401k_termination_withdrawal_or_rollover"
```

**Output esperado:**
```
‚úÖ Art√≠culo encontrado en Pinecone
  Total chunks: 33
  CRITICAL: 9 chunks
  HIGH: 10 chunks
  MEDIUM: 5 chunks
  LOW: 9 chunks
‚úÖ Todos los chunks tienen el mismo article_id
‚úÖ Todos los chunks cr√≠ticos presentes
```

---

## Pr√≥ximos Scripts a Crear

Una vez funcionando el upload, crear:

### 1. `scripts/load_all_articles.py`

Procesar todos los art√≠culos del directorio.

```bash
python scripts/load_all_articles.py \
  --directory "../Participant Advisory"
```

### 2. `scripts/delete_article_chunks.py`

Eliminar chunks de un art√≠culo.

```bash
python scripts/delete_article_chunks.py "article_id"
```

### 3. `scripts/update_article.py`

Actualizar un art√≠culo existente (delete + process).

```bash
python scripts/update_article.py "../Path/To/ARTICLE.json"
```

---

## Referencia: Formato Pinecone con Embeddings Integrados

### √çndice con Embeddings Integrados

```bash
pc index create \
  --name "my-index" \
  --metric "cosine" \
  --cloud "aws" \
  --region "us-east-1" \
  --model "llama-text-embed-v2" \      # ‚Üê Modelo de embeddings
  --field-map "text=content"            # ‚Üê Campo a embed
```

### Upsert con Embeddings Integrados

```python
# Formato CORRECTO
index.upsert(
    vectors=[
        {
            "id": "chunk_1",
            "data": {
                "content": "This text will be embedded"  # ‚Üê Pinecone embedir√° esto
            },
            "metadata": {
                "article_id": "...",
                "chunk_type": "..."
            }
        }
    ],
    namespace="kb_articles"
)
```

### Query con Embeddings Integrados

```python
# Query tambi√©n usa el campo de texto
results = index.query(
    data={"content": "search query text"},  # ‚Üê Pinecone embedir√° esto
    top_k=10,
    include_metadata=True,
    namespace="kb_articles",
    filter={"article_id": {"$eq": "..."}}
)
```

---

## Troubleshooting

### Si el error persiste:

1. **Verificar versi√≥n de Pinecone SDK:**
```bash
pip show pinecone
# Debe ser >= 5.0.0
```

2. **Verificar configuraci√≥n del √≠ndice:**
```bash
pc index describe --name kb-articles-production
# Verificar que tenga Model y Field Map
```

3. **Recrear √≠ndice si es necesario:**
```bash
pc index delete --name kb-articles-production
bash scripts/setup_index.sh
```

---

## Estado Actual

- ‚úÖ √çndice creado y configurado
- ‚úÖ Scripts base creados
- ‚úÖ Chunking funciona (33 chunks generados)
- ‚ö†Ô∏è  Upload necesita correcci√≥n (c√≥digo provisto arriba)
- ‚è≥ Scripts adicionales pendientes

---

## Pr√≥ximo Paso

1. **Aplicar correcci√≥n** en `pinecone_uploader.py`
2. **Probar upload** con art√≠culo de ejemplo
3. **Verificar** que chunks est√©n en Pinecone
4. **Crear scripts adicionales** del pipeline
5. **Pasar a Fase 5:** Implementaci√≥n del RAG Engine

---

**Progreso:** 80% completado  
**Bloqueador actual:** Formato de upsert (soluci√≥n provista)  
**Siguiente fase:** PHASE_5.md (RAG Engine)
