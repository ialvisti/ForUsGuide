# Gu√≠a del Pipeline - Procesamiento de Art√≠culos

## üìã Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Pipeline Autom√°tico](#pipeline-autom√°tico)
3. [Procesar un Art√≠culo Nuevo](#procesar-un-art√≠culo-nuevo)
4. [Procesar M√∫ltiples Art√≠culos](#procesar-m√∫ltiples-art√≠culos)
5. [Actualizar un Art√≠culo Existente](#actualizar-un-art√≠culo-existente)
6. [Eliminar un Art√≠culo](#eliminar-un-art√≠culo)
7. [Scripts Disponibles](#scripts-disponibles)
8. [Configuraci√≥n Avanzada](#configuraci√≥n-avanzada)
9. [Troubleshooting](#troubleshooting)

---

## Introducci√≥n

Este documento explica c√≥mo procesar art√≠culos JSON de la Knowledge Base y subirlos a Pinecone. El sistema est√° dise√±ado para ser **simple y automatizado**.

### ¬øQu√© hace el Pipeline?

```
Art√≠culo JSON ‚Üí Chunking ‚Üí Embeddings ‚Üí Pinecone
```

1. **Lee** el art√≠culo JSON
2. **Genera** ~30-35 chunks con metadata
3. **Crea** embeddings autom√°ticamente (Pinecone lo hace)
4. **Sube** los chunks a Pinecone
5. **Valida** que se subieron correctamente

---

## Pipeline Autom√°tico

### Opci√≥n 1: Procesar UN art√≠culo nuevo

```bash
cd kb-rag-system

# Activar virtual environment
source venv/bin/activate

# Ejecutar pipeline para un art√≠culo
python scripts/process_single_article.py "../Participant Advisory/Distributions/NEW_ARTICLE.json"
```

**Output esperado:**
```
‚úÖ Art√≠culo cargado: [T√≠tulo del art√≠culo]
üî® Generando chunks...
‚úÖ 33 chunks generados
üì§ Subiendo a Pinecone...
‚úÖ Chunks subidos exitosamente
‚úÖ Pipeline completado
```

---

### Opci√≥n 2: Procesar TODOS los art√≠culos

```bash
cd kb-rag-system
source venv/bin/activate

# Procesar todos los .json en directorio
python scripts/load_all_articles.py
```

**Output esperado:**
```
üìÇ Buscando art√≠culos en: ../Participant Advisory/
‚úÖ Encontrados 280 art√≠culos

Procesando: [1/280] LT: How to Request...
  ‚úÖ 33 chunks generados
  ‚úÖ Subidos a Pinecone

Procesando: [2/280] Vanguard: Loan Application...
  ‚úÖ 31 chunks generados
  ‚úÖ Subidos a Pinecone

...

‚úÖ COMPLETADO
  Total art√≠culos: 280
  Total chunks: 8,523
  Tiempo: 12m 34s
```

---

### Opci√≥n 3: Modo Watch (Autom√°tico)

Para entornos de producci√≥n, puedes dejar un proceso corriendo que detecte art√≠culos nuevos autom√°ticamente:

```bash
cd kb-rag-system
source venv/bin/activate

# Monitorear directorio y procesar autom√°ticamente
python scripts/watch_articles.py --directory "../Participant Advisory" --interval 60
```

**Qu√© hace:**
- Monitorea el directorio cada 60 segundos
- Detecta archivos `.json` nuevos o modificados
- Los procesa autom√°ticamente
- Loguea todo en `logs/pipeline.log`

---

## Procesar un Art√≠culo Nuevo

### Paso a Paso

#### 1. Crear el art√≠culo JSON

Coloca tu nuevo art√≠culo en el directorio apropiado:

```
Participant Advisory/
  ‚îî‚îÄ‚îÄ Distributions/
      ‚îî‚îÄ‚îÄ NUEVO_ARTICULO.json
```

**Requisitos del JSON:**
- Debe tener las secciones: `metadata`, `summary`, `details`
- Debe incluir: `article_id`, `title`, `record_keeper`, `plan_type`

#### 2. Validar estructura

```bash
python scripts/validate_article.py "../Participant Advisory/Distributions/NUEVO_ARTICULO.json"
```

**Output si es v√°lido:**
```
‚úÖ Estructura v√°lida
‚úÖ Metadata completa
‚úÖ Secciones requeridas presentes
```

#### 3. Procesar y subir

```bash
python scripts/process_single_article.py "../Participant Advisory/Distributions/NUEVO_ARTICULO.json"
```

#### 4. Verificar en Pinecone

```bash
python scripts/verify_article.py "article_id_del_nuevo_articulo"
```

**Output esperado:**
```
‚úÖ Art√≠culo encontrado en Pinecone
  Chunks: 33
  Record keeper: LT Trust
  Plan type: 401(k)
  
Chunks por tier:
  CRITICAL: 9
  HIGH: 10
  MEDIUM: 5
  LOW: 9
```

---

## Procesar M√∫ltiples Art√≠culos

### Escenario: Tienes 50 art√≠culos nuevos

```bash
# Opci√≥n A: Procesar todos de una vez
python scripts/load_all_articles.py --directory "../Participant Advisory/Distributions" --new-only

# Opci√≥n B: Procesar por lotes (m√°s seguro)
python scripts/batch_process.py --directory "../Participant Advisory/Distributions" --batch-size 10
```

**Ventajas del procesamiento por lotes:**
- Si falla uno, los dem√°s contin√∫an
- Menos carga en Pinecone
- Mejor logging y tracking
- M√°s f√°cil de pausar/reanudar

---

## Actualizar un Art√≠culo Existente

### ¬øCu√°ndo actualizar?

- El contenido del art√≠culo cambi√≥
- Se agregaron nuevas secciones
- Se corrigieron errores
- Se actualizaron fees o reglas

### Proceso de Actualizaci√≥n

#### 1. Modificar el art√≠culo JSON

Edita el archivo JSON con los cambios necesarios.

#### 2. Eliminar chunks antiguos

```bash
python scripts/delete_article_chunks.py "article_id_del_articulo"
```

**Output:**
```
üîç Buscando chunks de: article_id_del_articulo
‚úÖ Encontrados 33 chunks
üóëÔ∏è  Eliminando chunks...
‚úÖ 33 chunks eliminados
```

#### 3. Procesar versi√≥n actualizada

```bash
python scripts/process_single_article.py "../Path/To/ARTICULO_ACTUALIZADO.json"
```

#### 4. Verificar actualizaci√≥n

```bash
python scripts/verify_article.py "article_id_del_articulo"
```

### Actualizaci√≥n Autom√°tica (Un solo comando)

```bash
python scripts/update_article.py "../Path/To/ARTICULO_ACTUALIZADO.json"
```

Esto hace internamente:
1. Lee el `article_id` del JSON
2. Elimina chunks antiguos
3. Procesa y sube nueva versi√≥n
4. Verifica que todo est√© OK

---

## Eliminar un Art√≠culo

### ¬øCu√°ndo eliminar?

- El art√≠culo ya no es v√°lido
- Se deprec√≥ por uno nuevo
- Conten√≠a informaci√≥n incorrecta
- Plan o recordkeeper se descontinu√≥

### Proceso de Eliminaci√≥n

```bash
# Eliminar por article_id
python scripts/delete_article_chunks.py "article_id_a_eliminar"

# O eliminar por archivo (lee el article_id del JSON)
python scripts/delete_article_by_file.py "../Path/To/ARTICULO.json"
```

**Confirmaci√≥n:**
```
‚ö†Ô∏è  ADVERTENCIA: Esto eliminar√° todos los chunks de:
  Article ID: lt_request_401k_withdrawal
  Title: LT: How to Request a 401(k)...
  Chunks estimados: 33

¬øContinuar? (yes/no): yes

üóëÔ∏è  Eliminando...
‚úÖ 33 chunks eliminados exitosamente
```

---

## Scripts Disponibles

### Tabla R√°pida

| Script | Prop√≥sito | Uso |
|--------|-----------|-----|
| `process_single_article.py` | Procesar 1 art√≠culo | Art√≠culos nuevos |
| `load_all_articles.py` | Procesar todos | Setup inicial o bulk update |
| `update_article.py` | Actualizar art√≠culo existente | Cambios en contenido |
| `delete_article_chunks.py` | Eliminar chunks de art√≠culo | Deprecaci√≥n |
| `validate_article.py` | Validar estructura JSON | Pre-procesamiento |
| `verify_article.py` | Verificar en Pinecone | Post-procesamiento |
| `watch_articles.py` | Monitorear y auto-procesar | Producci√≥n |
| `batch_process.py` | Procesar por lotes | Bulk seguro |
| `list_articles_in_pinecone.py` | Listar art√≠culos en DB | Inventario |
| `reprocess_failed.py` | Reprocesar fallos | Recovery |

---

### 1. `process_single_article.py`

**Uso:**
```bash
python scripts/process_single_article.py <path-to-json>

# Ejemplos
python scripts/process_single_article.py "../Participant Advisory/Distributions/NEW.json"
python scripts/process_single_article.py --file "../Loans/LOAN_ARTICLE.json" --dry-run
```

**Opciones:**
- `--dry-run` - Solo genera chunks, no sube a Pinecone
- `--verbose` - Output detallado
- `--show-chunks` - Muestra chunks generados

---

### 2. `load_all_articles.py`

**Uso:**
```bash
python scripts/load_all_articles.py [--directory <dir>] [--new-only] [--skip-existing]

# Ejemplos
python scripts/load_all_articles.py
python scripts/load_all_articles.py --directory "../Participant Advisory/Loans"
python scripts/load_all_articles.py --new-only --skip-existing
```

**Opciones:**
- `--directory` - Directorio a escanear (default: ../Participant Advisory)
- `--new-only` - Solo art√≠culos no procesados previamente
- `--skip-existing` - Skip art√≠culos ya en Pinecone
- `--parallel` - Procesar en paralelo (m√°s r√°pido)

---

### 3. `update_article.py`

**Uso:**
```bash
python scripts/update_article.py <path-to-json>

# Ejemplo
python scripts/update_article.py "../Participant Advisory/Distributions/UPDATED.json"
```

**Proceso:**
1. Extrae `article_id` del JSON
2. Busca y elimina chunks existentes
3. Genera nuevos chunks
4. Sube a Pinecone
5. Verifica

---

### 4. `watch_articles.py`

**Uso:**
```bash
python scripts/watch_articles.py --directory <dir> --interval <seconds>

# Ejemplo
python scripts/watch_articles.py --directory "../Participant Advisory" --interval 60
```

**Qu√© monitorea:**
- Archivos `.json` nuevos ‚Üí Procesa autom√°ticamente
- Archivos `.json` modificados ‚Üí Actualiza autom√°ticamente
- Archivos `.json` eliminados ‚Üí Elimina chunks de Pinecone

**Log:**
```
[2026-01-18 10:30:15] INFO: Monitoring: ../Participant Advisory
[2026-01-18 10:31:20] INFO: New file detected: NEW_ARTICLE.json
[2026-01-18 10:31:22] INFO: Processing...
[2026-01-18 10:31:45] INFO: ‚úÖ Processed successfully (33 chunks)
[2026-01-18 10:32:30] INFO: Modified file detected: EXISTING.json
[2026-01-18 10:32:32] INFO: Updating...
[2026-01-18 10:32:55] INFO: ‚úÖ Updated successfully (33 chunks)
```

---

### 5. `verify_article.py`

**Uso:**
```bash
python scripts/verify_article.py <article_id>

# Ejemplo
python scripts/verify_article.py "lt_request_401k_termination_withdrawal_or_rollover"
```

**Output:**
```
üîç Verificando art√≠culo: lt_request_401k...

‚úÖ Art√≠culo encontrado en Pinecone

Informaci√≥n:
  Title: LT: How to Request a 401(k)...
  Record keeper: LT Trust
  Plan type: 401(k)
  Total chunks: 33

Chunks por tier:
  CRITICAL: 9 chunks
    - required_data (data_collection)
    - eligibility (requirements)
    - critical_flags (validation)
    ...
  
  HIGH: 10 chunks
    - steps (steps_1_to_3)
    - fees_details (costs)
    ...

Metadata consistente: ‚úÖ
Todos los chunks tienen article_id correcto: ‚úÖ
```

---

## Configuraci√≥n Avanzada

### Variables de Entorno (`.env`)

```bash
# Pinecone
PINECONE_API_KEY=your-api-key
INDEX_NAME=kb-articles-production
NAMESPACE=kb_articles

# Processing
BATCH_SIZE=96                 # Max chunks por batch (Pinecone limit)
MAX_RETRIES=3                 # Reintentos si falla upload
RETRY_DELAY=2                 # Segundos entre reintentos

# Monitoring
LOG_LEVEL=INFO                # DEBUG, INFO, WARNING, ERROR
LOG_FILE=logs/pipeline.log

# Watch mode
WATCH_INTERVAL=60             # Segundos entre scans
WATCH_RECURSIVE=true          # Buscar en subdirectorios
```

---

### Configuraci√≥n de Chunking

Si necesitas ajustar el chunking (archivo `data_pipeline/chunking.py`):

```python
# Cambiar tama√±o de agrupaci√≥n de steps
def _group_steps(self, steps):
    chunk_size = 3  # ‚Üê Cambiar aqu√≠ (default: 3 pasos por chunk)
    ...

# Cambiar agrupaci√≥n de FAQs
def _group_faqs(self, faqs):
    chunk_size = 3  # ‚Üê Cambiar aqu√≠ (default: 3 FAQs por chunk)
    ...
```

**Consideraciones:**
- Chunks m√°s peque√±os ‚Üí B√∫squeda m√°s precisa, pero m√°s vectores
- Chunks m√°s grandes ‚Üí Menos vectores, pero b√∫squeda menos precisa
- Balance recomendado: 200-500 palabras por chunk

---

### Configuraci√≥n de Batch Processing

Archivo `scripts/batch_process.py`:

```python
# Configurar tama√±o de lotes
BATCH_SIZE = 10  # Procesar 10 art√≠culos a la vez

# Configurar paralelismo
MAX_WORKERS = 4  # Procesar 4 art√≠culos en paralelo

# Configurar delay entre lotes
BATCH_DELAY = 5  # 5 segundos entre lotes
```

---

## Troubleshooting

### Problema 1: "Article not found"

**S√≠ntoma:**
```
‚ùå Error: Archivo no encontrado: ../Path/To/ARTICLE.json
```

**Soluci√≥n:**
- Verifica la ruta del archivo
- Usa rutas absolutas si hay duda:
  ```bash
  python scripts/process_single_article.py "/Users/user/Desktop/FUA Knowledge Base Articles/Participant Advisory/Distributions/ARTICLE.json"
  ```

---

### Problema 2: "Invalid JSON structure"

**S√≠ntoma:**
```
‚ùå Error: Art√≠culo inv√°lido
  Secci√≥n faltante: metadata
```

**Soluci√≥n:**
1. Validar estructura:
   ```bash
   python scripts/validate_article.py "../Path/To/ARTICLE.json"
   ```
2. Asegurar que el JSON tiene:
   - `metadata` con `article_id`, `title`, `record_keeper`, `plan_type`
   - `summary`
   - `details`

---

### Problema 3: "Pinecone connection failed"

**S√≠ntoma:**
```
‚ùå Error: Failed to connect to Pinecone
  Status code: 401
```

**Soluci√≥n:**
1. Verificar API key en `.env`:
   ```bash
   cat .env | grep PINECONE_API_KEY
   ```
2. Verificar que el √≠ndice existe:
   ```bash
   python scripts/check_pinecone_connection.py
   ```
3. Si el √≠ndice no existe, crearlo:
   ```bash
   bash scripts/setup_index.sh
   ```

---

### Problema 4: "Batch upload failed"

**S√≠ntoma:**
```
‚ùå Error: Batch upload failed
  Failed chunks: 15/96
```

**Soluci√≥n:**
1. Revisar tama√±o de chunks (no debe exceder 2MB por batch)
2. Reducir batch size en `.env`:
   ```bash
   BATCH_SIZE=50  # Reducir de 96 a 50
   ```
3. Reintentar con:
   ```bash
   python scripts/reprocess_failed.py
   ```

---

### Problema 5: "Duplicate chunks detected"

**S√≠ntoma:**
```
‚ö†Ô∏è  Warning: Duplicate chunks detected
  Article: lt_request_401k...
  Duplicates: 33 chunks already exist
```

**Soluci√≥n:**
1. Si quieres reemplazar, eliminar primero:
   ```bash
   python scripts/delete_article_chunks.py "article_id"
   python scripts/process_single_article.py "../Path/To/ARTICLE.json"
   ```
2. Si quieres mantener existentes, usar `--skip-existing`:
   ```bash
   python scripts/load_all_articles.py --skip-existing
   ```

---

### Problema 6: "Out of memory"

**S√≠ntoma:**
```
‚ùå Error: MemoryError
  Processing large batch...
```

**Soluci√≥n:**
1. Procesar en lotes m√°s peque√±os:
   ```bash
   python scripts/batch_process.py --batch-size 5
   ```
2. Deshabilitar paralelismo:
   ```bash
   python scripts/load_all_articles.py --no-parallel
   ```

---

## Best Practices

### 1. Siempre Validar Antes de Procesar

```bash
# Validar primero
python scripts/validate_article.py "../Path/To/NEW.json"

# Si es v√°lido, procesar
python scripts/process_single_article.py "../Path/To/NEW.json"
```

### 2. Usar Dry-Run para Testing

```bash
# Ver qu√© chunks se generar√≠an sin subir a Pinecone
python scripts/process_single_article.py "../Path/To/NEW.json" --dry-run --show-chunks
```

### 3. Backup Antes de Bulk Updates

```bash
# Exportar art√≠culos actuales de Pinecone
python scripts/export_all_chunks.py --output backup_$(date +%Y%m%d).json

# Luego procesar
python scripts/load_all_articles.py
```

### 4. Monitorear Logs en Producci√≥n

```bash
# Tail logs en tiempo real
tail -f logs/pipeline.log

# Buscar errores
grep "ERROR" logs/pipeline.log

# Ver resumen
python scripts/analyze_logs.py
```

### 5. Verificar Despu√©s de Cambios

```bash
# Despu√©s de actualizar un art√≠culo
python scripts/verify_article.py "article_id"

# Despu√©s de bulk update
python scripts/verify_all_articles.py
```

---

## Integraci√≥n con CI/CD

### Ejemplo: GitHub Actions

```yaml
name: Process New KB Articles

on:
  push:
    paths:
      - 'Participant Advisory/**/*.json'

jobs:
  process-articles:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          cd kb-rag-system
          pip install -r requirements.txt
      
      - name: Process changed articles
        env:
          PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
        run: |
          cd kb-rag-system
          python scripts/process_changed_articles.py
```

---

## Resumen de Comandos Frecuentes

```bash
# Setup inicial (una vez)
bash scripts/setup_index.sh
python scripts/load_all_articles.py

# Art√≠culo nuevo
python scripts/process_single_article.py "../Path/To/NEW.json"

# Actualizar art√≠culo
python scripts/update_article.py "../Path/To/UPDATED.json"

# Eliminar art√≠culo
python scripts/delete_article_chunks.py "article_id"

# Verificar art√≠culo
python scripts/verify_article.py "article_id"

# Modo watch (producci√≥n)
python scripts/watch_articles.py --directory "../Participant Advisory" --interval 60
```

---

**Pr√≥ximos Pasos:** Ver `ARCHITECTURE.md` para entender c√≥mo funciona el sistema completo.
